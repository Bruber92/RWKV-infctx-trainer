{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the datapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "DEEPSPEED_STRAT: deepspeed_stage_2\n",
      "TRAINING_CTX_LEN: 4096\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-run/chunk-1\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Your configurable settings\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# WANDB settings\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"RWKV-v5-Finetune\"\n",
    "WANDB_PROJECT=\"RWKV-v5-Finetune\"\n",
    "\n",
    "# Project directory offset (you need to modify if, you move the notebook into another dir)\n",
    "PROJECT_DIR_OFFSET=\"../../../../\"\n",
    "\n",
    "# Config dir (relative to the notebook, excluding ending slash)\n",
    "# to use, with the config filename\n",
    "CONFIG_FILE_DIR=\".\"\n",
    "CONFIG_FILE_NAME=\"Eagle-x-zMultipack-Instruct\"\n",
    "\n",
    "# The model to use\n",
    "MODEL_NAME=\"RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\"\n",
    "MODEL_URL=\"https://huggingface.co/RWKV/v5-Eagle-7B/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth?download=true\"\n",
    "\n",
    "# GPU count to use\n",
    "GPU_DEVICES=\"auto\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# # Training settings you can use to override the \"auto\" default above\n",
    "# -----------------------------------------------------------------\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "TRAINING_CTX_LEN=4096\n",
    "MICROBATCH_SIZE=8\n",
    "\n",
    "# ---\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"TRAINING_CTX_LEN:\", TRAINING_CTX_LEN)\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, PROJECT_DIR_OFFSET))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(TRAINER_DIR):\n",
    "    raise Exception(\"The trainer directory does not exists. Did you move the notebook?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-run/chunk-1/datapack-build.yaml\n",
      ">> Preparing dataset - index:  0  - name:  books_0\n",
      ">> Preparing dataset - index:  1  - name:  code_0\n",
      ">> Preparing dataset - index:  2  - name:  code_1\n",
      ">> Preparing dataset - index:  3  - name:  code_10\n",
      ">> Preparing dataset - index:  4  - name:  code_11\n",
      ">> Preparing dataset - index:  5  - name:  code_12\n",
      ">> Preparing dataset - index:  6  - name:  code_13\n",
      ">> Preparing dataset - index:  7  - name:  code_14\n",
      ">> Preparing dataset - index:  8  - name:  code_15\n",
      ">> Preparing dataset - index:  9  - name:  code_16\n",
      ">> Preparing dataset - index:  10  - name:  code_17\n",
      ">> Preparing dataset - index:  11  - name:  code_18\n",
      ">> Preparing dataset - index:  12  - name:  code_19\n",
      ">> Preparing dataset - index:  13  - name:  code_2\n",
      ">> Preparing dataset - index:  14  - name:  code_20\n",
      ">> Preparing dataset - index:  15  - name:  code_21\n",
      ">> Preparing dataset - index:  16  - name:  code_3\n",
      ">> Preparing dataset - index:  17  - name:  code_4\n",
      ">> Preparing dataset - index:  18  - name:  code_5\n",
      ">> Preparing dataset - index:  19  - name:  code_6\n",
      ">> Preparing dataset - index:  20  - name:  code_7\n",
      ">> Preparing dataset - index:  21  - name:  code_8\n",
      ">> Preparing dataset - index:  22  - name:  code_9\n",
      ">> Preparing dataset - index:  23  - name:  law_0\n",
      ">> Preparing dataset - index:  24  - name:  law_1\n",
      ">> Preparing dataset - index:  25  - name:  law_2\n",
      ">> Preparing dataset - index:  26  - name:  news_0\n",
      ">> Preparing dataset - index:  27  - name:  papers-split-aa_0\n",
      ">> Preparing dataset - index:  28  - name:  papers-split-aa_1\n",
      ">> Preparing dataset - index:  29  - name:  papers-split-ab_0\n",
      ">> Preparing dataset - index:  30  - name:  papers-split-ab_1\n",
      ">> Preparing dataset - index:  31  - name:  papers-split-ab_2\n",
      ">> Preparing dataset - index:  32  - name:  papers-split-ab_3\n",
      ">> Preparing dataset - index:  33  - name:  papers-split-ac_0\n",
      ">> Preparing dataset - index:  34  - name:  papers-split-ac_1\n",
      ">> Preparing dataset - index:  35  - name:  papers-split-ad_0\n",
      ">> Preparing dataset - index:  36  - name:  papers-split-ad_1\n",
      ">> Preparing dataset - index:  37  - name:  papers-split-ae_0\n",
      ">> Preparing dataset - index:  38  - name:  papers-split-ae_1\n",
      ">> Preparing dataset - index:  39  - name:  papers-split-ae_10\n",
      ">> Preparing dataset - index:  40  - name:  papers-split-ae_11\n",
      ">> Preparing dataset - index:  41  - name:  papers-split-ae_12\n",
      ">> Preparing dataset - index:  42  - name:  papers-split-ae_13\n",
      ">> Preparing dataset - index:  43  - name:  papers-split-ae_14\n",
      ">> Preparing dataset - index:  44  - name:  papers-split-ae_15\n",
      ">> Preparing dataset - index:  45  - name:  papers-split-ae_16\n",
      ">> Preparing dataset - index:  46  - name:  papers-split-ae_17\n",
      ">> Preparing dataset - index:  47  - name:  papers-split-ae_2\n",
      ">> Preparing dataset - index:  48  - name:  papers-split-ae_3\n",
      ">> Preparing dataset - index:  49  - name:  papers-split-ae_4\n",
      ">> Preparing dataset - index:  50  - name:  papers-split-ae_5\n",
      ">> Preparing dataset - index:  51  - name:  papers-split-ae_6\n",
      ">> Preparing dataset - index:  52  - name:  papers-split-ae_7\n",
      ">> Preparing dataset - index:  53  - name:  papers-split-ae_8\n",
      ">> Preparing dataset - index:  54  - name:  papers-split-ae_9\n",
      ">> Preparing dataset - index:  55  - name:  papers-split-af_0\n",
      ">> Preparing dataset - index:  56  - name:  papers-split-af_1\n",
      ">> Preparing dataset - index:  57  - name:  papers-split-af_2\n",
      ">> Preparing dataset - index:  58  - name:  qna_0\n",
      ">> Preparing dataset - index:  59  - name:  qna_1\n",
      ">> Preparing dataset - index:  60  - name:  qna_10\n",
      ">> Preparing dataset - index:  61  - name:  qna_11\n",
      ">> Preparing dataset - index:  62  - name:  qna_12\n",
      ">> Preparing dataset - index:  63  - name:  qna_13\n",
      ">> Preparing dataset - index:  64  - name:  qna_14\n",
      ">> Preparing dataset - index:  65  - name:  qna_15\n",
      ">> Preparing dataset - index:  66  - name:  qna_16\n",
      ">> Preparing dataset - index:  67  - name:  qna_2\n",
      ">> Preparing dataset - index:  68  - name:  qna_3\n",
      ">> Preparing dataset - index:  69  - name:  qna_4\n",
      ">> Preparing dataset - index:  70  - name:  qna_5\n",
      ">> Preparing dataset - index:  71  - name:  qna_6\n",
      ">> Preparing dataset - index:  72  - name:  qna_7\n",
      ">> Preparing dataset - index:  73  - name:  qna_8\n",
      ">> Preparing dataset - index:  74  - name:  qna_9\n",
      ">> Preparing dataset - index:  75  - name:  various_0\n",
      ">> Preparing dataset - index:  76  - name:  various_1\n",
      ">> Preparing dataset - index:  77  - name:  various_2\n",
      ">> Preparing dataset - index:  78  - name:  various_3\n",
      ">> Preparing dataset - index:  79  - name:  wiki-split-aa_0\n",
      ">> Preparing dataset - index:  80  - name:  wiki-split-aa_1\n",
      ">> Preparing dataset - index:  81  - name:  wiki-split-ab_0\n",
      ">> Preparing dataset - index:  82  - name:  wiki-split-ab_1\n",
      ">> Preparing dataset - index:  83  - name:  wiki-split-ac_0\n",
      ">> Preparing dataset - index:  84  - name:  wiki-split-ac_1\n",
      ">> Preparing dataset - index:  85  - name:  wiki-split-ad_0\n",
      ">> Preparing dataset - index:  86  - name:  wiki-split-ad_1\n",
      ">> Preparing dataset - index:  87  - name:  wiki-split-ae_0\n",
      ">> Preparing dataset - index:  88  - name:  wiki-split-ae_1\n",
      ">> Preparing dataset - index:  89  - name:  wiki-split-af_0\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:14, 20527.37 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:27<00:00, 10720.52 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:14<00:00, 20586.15 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:26<00:00, 11162.57 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 86041/86041 [00:18<00:00, 4544.80 examples/s]\n",
      "Saving the dataset (17/17 shards): 100%|█| 86041/86041 [00:23<00:00, 3633.48 exa\n",
      "Saving the dataset (1/1 shards): 100%|█| 87/87 [00:00<00:00, 1707.63 examples/s]\n",
      ">> Preparing dataset - index:  90  - name:  wiki-split-af_1\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 153162 examples [00:07, 19733.18 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 153162/153162 [00:14<00:00, 10867.74 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 153162/153162 [00:07<00:00, 19719.49 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 153162/153162 [00:13<00:00, 11176.43 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 43630/43630 [00:08<00:00, 4887.64 examples/s]\n",
      "Saving the dataset (9/9 shards): 100%|█| 43630/43630 [00:11<00:00, 3965.03 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 44/44 [00:00<00:00, 1069.19 examples/s]\n",
      ">> Preparing dataset - index:  91  - name:  wiki-split-ag_0\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:11, 26695.75 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:20<00:00, 14813.00 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:10<00:00, 28089.52 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:18<00:00, 15827.02 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 61983/61983 [00:11<00:00, 5177.45 examples/s]\n",
      "Saving the dataset (13/13 shards): 100%|█| 61983/61983 [00:14<00:00, 4409.24 exa\n",
      "Saving the dataset (1/1 shards): 100%|█| 63/63 [00:00<00:00, 1548.66 examples/s]\n",
      ">> Preparing dataset - index:  92  - name:  wiki-split-ag_1\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 136889 examples [00:04, 27395.47 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 136889/136889 [00:09<00:00, 14104.89 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 136889/136889 [00:05<00:00, 27140.94 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 136889/136889 [00:08<00:00, 15854.43 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 28234/28234 [00:06<00:00, 4495.34 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|█| 28234/28234 [00:06<00:00, 4139.37 examp\n",
      "Saving the dataset (1/1 shards): 100%|██| 29/29 [00:00<00:00, 864.92 examples/s]\n",
      ">> Preparing dataset - index:  93  - name:  wiki-split-ah_0\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:11, 25936.70 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:21<00:00, 13649.33 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:11<00:00, 25313.75 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:21<00:00, 14186.63 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 69467/69467 [00:13<00:00, 5227.25 examples/s]\n",
      "Saving the dataset (14/14 shards): 100%|█| 69467/69467 [00:17<00:00, 3941.00 exa\n",
      "Saving the dataset (1/1 shards): 100%|█| 70/70 [00:00<00:00, 1435.39 examples/s]\n",
      ">> Preparing dataset - index:  94  - name:  wiki-split-ah_1\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 199440 examples [00:07, 25140.38 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 199440/199440 [00:15<00:00, 13085.23 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 199440/199440 [00:08<00:00, 24486.60 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 199439/199439 [00:14<00:00, 13851.54 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 46238/46238 [00:09<00:00, 5060.67 examples/s]\n",
      "Saving the dataset (10/10 shards): 100%|█| 46238/46238 [00:10<00:00, 4259.70 exa\n",
      "Saving the dataset (1/1 shards): 100%|█| 47/47 [00:00<00:00, 1248.00 examples/s]\n",
      ">> Preparing dataset - index:  95  - name:  wiki-split-ai_0\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:05, 55504.70 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:11<00:00, 26576.01 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:06<00:00, 46548.71 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 299997/299997 [00:10<00:00, 27980.91 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 34169/34169 [00:07<00:00, 4748.11 examples/s]\n",
      "Saving the dataset (7/7 shards): 100%|█| 34169/34169 [00:08<00:00, 4005.43 examp\n",
      "Saving the dataset (1/1 shards): 100%|██| 35/35 [00:00<00:00, 870.01 examples/s]\n",
      ">> Preparing dataset - index:  96  - name:  wiki-split-ai_1\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:05, 52553.67 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:11<00:00, 25508.94 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:06<00:00, 48448.74 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 299996/299996 [00:10<00:00, 28528.23 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 34442/34442 [00:07<00:00, 4912.09 examples/s]\n",
      "Saving the dataset (7/7 shards): 100%|█| 34442/34442 [00:08<00:00, 4169.52 examp\n",
      "Saving the dataset (1/1 shards): 100%|██| 35/35 [00:00<00:00, 927.77 examples/s]\n",
      ">> Preparing dataset - index:  97  - name:  wiki-split-ai_2\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 144321 examples [00:02, 52643.44 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 144321/144321 [00:05<00:00, 24820.54 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 144321/144321 [00:03<00:00, 42702.00 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 144319/144319 [00:05<00:00, 24249.53 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 16570/16570 [00:04<00:00, 3924.97 examples/s]\n",
      "Saving the dataset (4/4 shards): 100%|█| 16570/16570 [00:04<00:00, 4079.79 examp\n",
      "Saving the dataset (1/1 shards): 100%|██| 17/17 [00:00<00:00, 495.85 examples/s]\n",
      ">> Preparing dataset - index:  98  - name:  wiki-split-aj_0\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:06, 47426.96 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:13<00:00, 22925.67 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:06<00:00, 45306.97 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:11<00:00, 26378.89 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 36825/36825 [00:07<00:00, 4726.08 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|█| 36825/36825 [00:08<00:00, 4238.89 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 37/37 [00:00<00:00, 1087.33 examples/s]\n",
      ">> Preparing dataset - index:  99  - name:  wiki-split-aj_1\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:06, 45295.60 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:12<00:00, 23260.74 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:06<00:00, 44147.74 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:11<00:00, 26473.07 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 37124/37124 [00:07<00:00, 5059.11 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|█| 37124/37124 [00:08<00:00, 4349.23 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 38/38 [00:00<00:00, 1147.82 examples/s]\n",
      ">> Preparing dataset - index:  100  - name:  wiki-split-aj_2\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 129256 examples [00:02, 45519.83 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 129256/129256 [00:05<00:00, 22347.70 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 129256/129256 [00:03<00:00, 39310.12 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 129256/129256 [00:05<00:00, 24482.62 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 15635/15635 [00:03<00:00, 4590.57 examples/s]\n",
      "Saving the dataset (4/4 shards): 100%|█| 15635/15635 [00:03<00:00, 4110.56 examp\n",
      "Saving the dataset (1/1 shards): 100%|██| 16/16 [00:00<00:00, 551.19 examples/s]\n",
      ">> Preparing dataset - index:  101  - name:  wiki-split-ak_0\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:06, 48899.91 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:12<00:00, 24348.24 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:06<00:00, 43293.09 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 299998/299998 [00:12<00:00, 24679.55 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 38875/38875 [00:07<00:00, 5166.76 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|█| 38875/38875 [00:09<00:00, 4224.01 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 39/39 [00:00<00:00, 1076.20 examples/s]\n",
      ">> Preparing dataset - index:  102  - name:  wiki-split-ak_1\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300000 examples [00:06, 47337.27 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 300000/300000 [00:12<00:00, 23506.98 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 300000/300000 [00:06<00:00, 42982.32 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 299999/299999 [00:15<00:00, 19528.19 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 39487/39487 [00:07<00:00, 5071.94 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|█| 39487/39487 [00:09<00:00, 3972.16 examp\n",
      "Saving the dataset (1/1 shards): 100%|██| 40/40 [00:00<00:00, 996.17 examples/s]\n",
      ">> Preparing dataset - index:  103  - name:  wiki-split-ak_2\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 286438 examples [00:05, 48140.16 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 286438/286438 [00:11<00:00, 24275.90 examples/s]\n",
      "Filter (num_proc=160): 100%|██| 286438/286438 [00:06<00:00, 42778.37 examples/s]\n",
      "Map (num_proc=160): 100%|█████| 286438/286438 [00:13<00:00, 21421.94 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 37441/37441 [00:07<00:00, 4904.16 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|█| 37441/37441 [00:09<00:00, 3985.56 examp\n",
      "Saving the dataset (1/1 shards): 100%|██| 38/38 [00:00<00:00, 987.37 examples/s]\n",
      ">> Dataset Mixing mode:  shuffle\n",
      ">> Saving dataset to data_path :  /datapath/2T-train-set/chunk-1/HFfull/2T-train-set/chunk-1/HFfull\n",
      "Saving the dataset (1/470 shards):   0%| | 9082/2388345 [00:37<2:41:50, 245.02 e"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/datapack-build.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
