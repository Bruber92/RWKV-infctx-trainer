{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eagle 7B : Finetuning on capybara chat!\n",
    "\n",
    "The following showcases an example of Training the RWKV-v5 7B model, on openhermes1 instruct dataset\n",
    "- https://huggingface.co/datasets/nampdn-ai/tiny-strange-textbooks\n",
    "\n",
    "In this example, we will be training the model with 16k packings sizes\n",
    "\n",
    "## Configure the env variable below\n",
    "The default auto strategy, should work on a single 4090, scaling up all the way to 8xH100s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_COUNT: 8\n",
      "GPU_0_VRAM_SIZE (GB): 79.10943603515625\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "DEEPSPEED_STRAT: deepspeed_stage_2\n",
      "TRAINING_CTX_LEN: 4096\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/finetune-example\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Your configurable settings\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# WANDB settings\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"RWKV-v5-Finetune\"\n",
    "WANDB_PROJECT=\"RWKV-v5-Finetune\"\n",
    "\n",
    "# Project directory offset (you need to modify if, you move the notebook into another dir)\n",
    "PROJECT_DIR_OFFSET=\"../../\"\n",
    "\n",
    "# Config dir (relative to the notebook, excluding ending slash)\n",
    "# to use, with the config filename\n",
    "CONFIG_FILE_DIR=\".\"\n",
    "CONFIG_FILE_NAME=\"Eagle-x-zMultipack\"\n",
    "\n",
    "# The model to use\n",
    "MODEL_NAME=\"RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\"\n",
    "MODEL_URL=\"https://huggingface.co/RWKV/v5-Eagle-7B/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth?download=true\"\n",
    "\n",
    "# GPU count to use\n",
    "GPU_DEVICES=\"auto\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Lets detect the GPU vram sizes, and suggest a resonable default\n",
    "# based on the detected VRAM sizes\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Default settings\n",
    "# NOTE: If your not using cuda, you may want to manually change this around\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "TRAINING_CTX_LEN=2048\n",
    "MICROBATCH_SIZE=1\n",
    "\n",
    "import torch\n",
    "if torch.cuda is None or not torch.cuda.is_available() or torch.cuda.device_count() <= 0:\n",
    "    print(\"No CUDA compatible GPU found, using default settings\")\n",
    "else:\n",
    "    # -----------------------------------------------------------------\n",
    "    # Auto select the strategy based on the detected VRAM size\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    GPU_COUNT=torch.cuda.device_count()\n",
    "    GPU_0_VRAM_SIZE_GB=torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if GPU_DEVICES != \"auto\":\n",
    "        GPU_COUNT=int(GPU_DEVICES)\n",
    "    print(\"GPU_COUNT:\", GPU_COUNT)\n",
    "    print(\"GPU_0_VRAM_SIZE (GB):\", GPU_0_VRAM_SIZE_GB)\n",
    "\n",
    "    if GPU_0_VRAM_SIZE_GB < 17:\n",
    "        assert False, \"For the Eagle-7B model, you need atleast 18GB vram\"\n",
    "    elif GPU_0_VRAM_SIZE_GB < 23:\n",
    "        # This takes about 17.5GB vram on a single GPU\n",
    "        # We DO NOT recommend training with ctx_len=128, as the training\n",
    "        # quality will degrade noticably. But it will work!\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "        TRAINING_CTX_LEN=128\n",
    "        MICROBATCH_SIZE=1\n",
    "    elif GPU_0_VRAM_SIZE_GB < 25:\n",
    "        # This takes about 21GB vram on a single GPU\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "        TRAINING_CTX_LEN=2048\n",
    "        MICROBATCH_SIZE=2\n",
    "    elif GPU_0_VRAM_SIZE_GB < 78:\n",
    "        # This takes about 23GB vram on a single GPU\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "        TRAINING_CTX_LEN=4096\n",
    "        MICROBATCH_SIZE=2\n",
    "        if GPU_COUNT >= 8:\n",
    "            MICROBATCH_SIZE=4\n",
    "    else:\n",
    "        # This is now the 80GB vram class\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "        TRAINING_CTX_LEN=4096\n",
    "        MICROBATCH_SIZE=4\n",
    "        if GPU_COUNT >= 8:\n",
    "            MICROBATCH_SIZE=8\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# # Training settings you can use to override the \"auto\" default above\n",
    "# -----------------------------------------------------------------\n",
    "# DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "# TRAINING_CTX_LEN=4096\n",
    "# MICROBATCH_SIZE=8\n",
    "\n",
    "# ---\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"TRAINING_CTX_LEN:\", TRAINING_CTX_LEN)\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, PROJECT_DIR_OFFSET))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(TRAINER_DIR):\n",
    "    raise Exception(\"The trainer directory does not exists. Did you move the notebook?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"./model\" && \\\n",
    "    cd \"./model\" && \\\n",
    "    wget -nc \"{MODEL_URL}\" -O \"{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/finetune-example/./Eagle-x-zMultipack-build.yaml\n",
      ">> Preparing dataset - index:  0\n",
      "Resolving data files: 100%|███████████████| 1267/1267 [00:00<00:00, 2538.68it/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (16/16 shards): 100%|█| 38611/38611 [00:15<00:00, 2438.45 exa\n",
      "Saving the dataset (1/1 shards): 100%|████| 8/8 [00:00<00:00, 428.06 examples/s]\n",
      ">> Preparing dataset - index:  1\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 3937.92 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1183.66 examples/s]\n",
      ">> Preparing dataset - index:  2\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (1/1 shards): 100%|█| 2656/2656 [00:00<00:00, 16478.54 exampl\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1189.28 examples/s]\n",
      ">> Preparing dataset - index:  3\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (20/20 shards): 100%|█| 187987/187987 [00:17<00:00, 10743.46 \n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1017.42 examples/s]\n",
      ">> Preparing dataset - index:  4\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (1/1 shards): 100%|█| 18344/18344 [00:00<00:00, 20859.03 exam\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1058.80 examples/s]\n",
      ">> Preparing dataset - index:  5\n",
      "Map (num_proc=160): 100%|███| 1001551/1001551 [00:14<00:00, 69453.18 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 1001551/1001551 [00:05<00:00, 199370.91 examples/\n",
      "Map (num_proc=160): 100%|██| 1001543/1001543 [00:06<00:00, 165501.37 examples/s]\n",
      "Map (num_proc=160): 100%|██| 1001543/1001543 [00:10<00:00, 100021.04 examples/s]\n",
      "Map (num_proc=160): 100%|███████| 82453/82453 [00:04<00:00, 17669.25 examples/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Map (num_proc=8): 100%|████████████████████| 8/8 [00:00<00:00, 33.43 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|█| 82453/82453 [00:04<00:00, 19651.30 exam\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1142.36 examples/s]\n",
      ">> Preparing dataset - index:  6\n",
      "Downloading readme: 100%|██████████████████| 6.47k/6.47k [00:00<00:00, 58.7MB/s]\n",
      "Downloading data: 100%|████████████████████| 74.0M/74.0M [00:06<00:00, 10.8MB/s]\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 16006 examples [00:00, 90567.09 examples/s]\n",
      "Map (num_proc=160): 100%|███████| 16006/16006 [00:01<00:00, 14156.75 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 16006/16006 [00:00<00:00, 23676.78 examples/s]\n",
      "Map (num_proc=160): 100%|███████| 15998/15998 [00:01<00:00, 11222.58 examples/s]\n",
      "Map (num_proc=160): 100%|███████| 15998/15998 [00:01<00:00, 12861.68 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 3715/3715 [00:00<00:00, 5670.04 examples/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Map (num_proc=8): 100%|████████████████████| 8/8 [00:00<00:00, 35.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 3715/3715 [00:00<00:00, 16505.38 exampl\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1108.07 examples/s]\n",
      ">> Preparing dataset - index:  7\n",
      "Downloading readme: 100%|██████████████████| 2.99k/2.99k [00:00<00:00, 37.3MB/s]\n",
      "Downloading data: 100%|████████████████████| 12.1M/12.1M [00:03<00:00, 3.28MB/s]\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 3857 examples [00:00, 92803.07 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 3857/3857 [00:00<00:00, 4714.82 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 3857/3857 [00:00<00:00, 6340.24 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 3849/3849 [00:01<00:00, 2716.03 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 3849/3849 [00:01<00:00, 3119.64 examples/s]\n",
      "Map (num_proc=160): 100%|████████████| 759/759 [00:00<00:00, 1180.38 examples/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Map (num_proc=8): 100%|████████████████████| 8/8 [00:00<00:00, 39.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 759/759 [00:00<00:00, 8056.97 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1209.69 examples/s]\n",
      ">> Total approx train batches ( full | random ) : 653  (  519  |  134  )\n",
      "Map (num_proc=160): 100%|██████| 334336/334336 [00:47<00:00, 7024.32 examples/s]\n",
      ">> Saving dataset to data_path :  ../datapath/world/Eagle-x-multipack/\n",
      "Saving the dataset (41/41 shards): 100%|█| 334685/334685 [00:41<00:00, 8096.97 e\n",
      "Saving the dataset (1/1 shards): 100%|█| 64/64 [00:00<00:00, 2855.00 examples/s]\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 334,685\n",
      ">> Final dataset count ( test  ) : 64\n",
      ">> -----------------------------------\n",
      "Map (num_proc=160): 100%|██████| 334685/334685 [01:04<00:00, 5215.53 examples/s]\n",
      "num_proc must be <= 64. Reducing num_proc to 64 for dataset of size 64.\n",
      "Map (num_proc=64): 100%|████████████████| 64/64 [00:00<00:00, 191.24 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 3,354,459,591\n",
      ">> - Valid tokens : 2,073,042,418\n",
      ">> - Hidden tokens : 1,281,417,173\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 281,239\n",
      ">> - Valid tokens : 276,273\n",
      ">> - Hidden tokens : 4,966\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/{CONFIG_FILE_DIR}/{CONFIG_FILE_NAME}-build.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the checkpoint dir\n",
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"./checkpoint/{CONFIG_FILE_NAME}/\"\n",
    "\n",
    "# Lets start the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/{CONFIG_FILE_DIR}/{CONFIG_FILE_NAME}.yaml\" \\\n",
    "        --model.load_model=\"../model/{MODEL_NAME}\" \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{CONFIG_FILE_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - {CONFIG_FILE_NAME} (tctxlen={TRAINING_CTX_LEN}, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.logger.init_args.project=\"{WANDB_PROJECT}\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.target_batch_size=64 \\\n",
    "        --trainer.microbatch_size={MICROBATCH_SIZE} \\\n",
    "        --model.ctx_len={TRAINING_CTX_LEN} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{CONFIG_FILE_NAME}/last.ckpt\" \"../model/{CONFIG_FILE_NAME}.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{CONFIG_FILE_NAME}.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check (that the model actually output stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 dragon_test.py \"../model/{CONFIG_FILE_NAME}.pth\" \"cuda bf16\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
