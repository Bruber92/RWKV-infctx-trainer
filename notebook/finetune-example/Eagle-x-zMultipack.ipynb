{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eagle 7B : Finetuning on capybara chat!\n",
    "\n",
    "The following showcases an example of Training the RWKV-v5 7B model, on openhermes1 instruct dataset\n",
    "- https://huggingface.co/datasets/nampdn-ai/tiny-strange-textbooks\n",
    "\n",
    "In this example, we will be training the model with 16k packings sizes\n",
    "\n",
    "## Configure the env variable below\n",
    "The default auto strategy, should work on a single 4090, scaling up all the way to 8xH100s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_COUNT: 8\n",
      "GPU_0_VRAM_SIZE (GB): 79.10943603515625\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "DEEPSPEED_STRAT: deepspeed_stage_2\n",
      "TRAINING_CTX_LEN: 4096\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/finetune-example\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Your configurable settings\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# WANDB settings\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"RWKV-v5-Finetune\"\n",
    "WANDB_PROJECT=\"RWKV-v5-Finetune\"\n",
    "\n",
    "# Project directory offset (you need to modify if, you move the notebook into another dir)\n",
    "PROJECT_DIR_OFFSET=\"../../\"\n",
    "\n",
    "# Config dir (relative to the notebook, excluding ending slash)\n",
    "# to use, with the config filename\n",
    "CONFIG_FILE_DIR=\".\"\n",
    "CONFIG_FILE_NAME=\"Eagle-x-zMultipack\"\n",
    "\n",
    "# The model to use\n",
    "MODEL_NAME=\"RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\"\n",
    "MODEL_URL=\"https://huggingface.co/RWKV/v5-Eagle-7B/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth?download=true\"\n",
    "\n",
    "# GPU count to use\n",
    "GPU_DEVICES=\"auto\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Lets detect the GPU vram sizes, and suggest a resonable default\n",
    "# based on the detected VRAM sizes\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Default settings\n",
    "# NOTE: If your not using cuda, you may want to manually change this around\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "TRAINING_CTX_LEN=2048\n",
    "MICROBATCH_SIZE=1\n",
    "\n",
    "import torch\n",
    "if torch.cuda is None or not torch.cuda.is_available() or torch.cuda.device_count() <= 0:\n",
    "    print(\"No CUDA compatible GPU found, using default settings\")\n",
    "else:\n",
    "    # -----------------------------------------------------------------\n",
    "    # Auto select the strategy based on the detected VRAM size\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    GPU_COUNT=torch.cuda.device_count()\n",
    "    GPU_0_VRAM_SIZE_GB=torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if GPU_DEVICES != \"auto\":\n",
    "        GPU_COUNT=int(GPU_DEVICES)\n",
    "    print(\"GPU_COUNT:\", GPU_COUNT)\n",
    "    print(\"GPU_0_VRAM_SIZE (GB):\", GPU_0_VRAM_SIZE_GB)\n",
    "\n",
    "    if GPU_0_VRAM_SIZE_GB < 17:\n",
    "        assert False, \"For the Eagle-7B model, you need atleast 18GB vram\"\n",
    "    elif GPU_0_VRAM_SIZE_GB < 23:\n",
    "        # This takes about 17.5GB vram on a single GPU\n",
    "        # We DO NOT recommend training with ctx_len=128, as the training\n",
    "        # quality will degrade noticably. But it will work!\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "        TRAINING_CTX_LEN=128\n",
    "        MICROBATCH_SIZE=1\n",
    "    elif GPU_0_VRAM_SIZE_GB < 25:\n",
    "        # This takes about 21GB vram on a single GPU\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "        TRAINING_CTX_LEN=2048\n",
    "        MICROBATCH_SIZE=2\n",
    "    elif GPU_0_VRAM_SIZE_GB < 78:\n",
    "        # This takes about 23GB vram on a single GPU\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "        TRAINING_CTX_LEN=4096\n",
    "        MICROBATCH_SIZE=2\n",
    "        if GPU_COUNT >= 8:\n",
    "            MICROBATCH_SIZE=4\n",
    "    else:\n",
    "        # This is now the 80GB vram class\n",
    "        DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "        TRAINING_CTX_LEN=4096\n",
    "        MICROBATCH_SIZE=4\n",
    "        if GPU_COUNT >= 8:\n",
    "            MICROBATCH_SIZE=8\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# # Training settings you can use to override the \"auto\" default above\n",
    "# -----------------------------------------------------------------\n",
    "# DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "# TRAINING_CTX_LEN=4096\n",
    "# MICROBATCH_SIZE=8\n",
    "\n",
    "# ---\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"TRAINING_CTX_LEN:\", TRAINING_CTX_LEN)\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, PROJECT_DIR_OFFSET))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(TRAINER_DIR):\n",
    "    raise Exception(\"The trainer directory does not exists. Did you move the notebook?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"./model\" && \\\n",
    "    cd \"./model\" && \\\n",
    "    wget -nc \"{MODEL_URL}\" -O \"{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/finetune-example/./Eagle-x-zMultipack-build.yaml\n",
      ">> Preparing dataset - index:  0\n",
      "Resolving data files: 100%|███████████████| 1267/1267 [00:00<00:00, 2769.84it/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (16/16 shards): 100%|█| 38611/38611 [00:16<00:00, 2306.08 exa\n",
      "Saving the dataset (1/1 shards): 100%|████| 8/8 [00:00<00:00, 469.88 examples/s]\n",
      ">> Preparing dataset - index:  1\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 4198.48 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1214.20 examples/s]\n",
      ">> Preparing dataset - index:  2\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (1/1 shards): 100%|█| 411/411 [00:00<00:00, 1797.83 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1132.14 examples/s]\n",
      ">> Preparing dataset - index:  3\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (20/20 shards): 100%|█| 48977/48977 [00:19<00:00, 2502.48 exa\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1037.10 examples/s]\n",
      ">> Preparing dataset - index:  4\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Saving the dataset (1/1 shards): 100%|█| 2394/2394 [00:00<00:00, 2401.14 example\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1026.57 examples/s]\n",
      ">> Preparing dataset - index:  5\n",
      "Downloading data: 100%|████████████████████| 1.94G/1.94G [01:21<00:00, 23.8MB/s]\n",
      "Setting num_proc from 160 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 1001551 examples [00:19, 52186.70 examples/s]\n",
      "Map (num_proc=160): 100%|███| 1001551/1001551 [00:13<00:00, 72873.61 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 1001551/1001551 [00:04<00:00, 200670.79 examples/\n",
      "Map (num_proc=160): 100%|██| 1001543/1001543 [00:07<00:00, 133141.86 examples/s]\n",
      "Map (num_proc=160): 100%|██| 1001543/1001543 [00:09<00:00, 103921.25 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 11780/11780 [00:04<00:00, 2502.38 examples/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Map (num_proc=8): 100%|████████████████████| 8/8 [00:00<00:00, 27.16 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|█| 11780/11780 [00:05<00:00, 2347.79 examp\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1008.91 examples/s]\n",
      ">> Preparing dataset - index:  6\n",
      "Map (num_proc=160): 100%|███████| 16006/16006 [00:01<00:00, 12755.01 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 16006/16006 [00:00<00:00, 23321.48 examples/s]\n",
      "Map (num_proc=160): 100%|███████| 15998/15998 [00:01<00:00, 11351.40 examples/s]\n",
      "Map (num_proc=160): 100%|███████| 15998/15998 [00:01<00:00, 12205.94 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 548/548 [00:00<00:00, 849.48 examples/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Map (num_proc=8): 100%|████████████████████| 8/8 [00:00<00:00, 31.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 548/548 [00:00<00:00, 1767.44 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1013.06 examples/s]\n",
      ">> Preparing dataset - index:  7\n",
      "Map (num_proc=160): 100%|██████████| 3857/3857 [00:00<00:00, 4524.76 examples/s]\n",
      "Filter (num_proc=160): 100%|███████| 3857/3857 [00:00<00:00, 6157.25 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 3849/3849 [00:01<00:00, 2682.75 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 3849/3849 [00:01<00:00, 3181.50 examples/s]\n",
      "Map (num_proc=160): 100%|█████████████| 160/160 [00:00<00:00, 245.60 examples/s]\n",
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n",
      "Map (num_proc=8): 100%|████████████████████| 8/8 [00:00<00:00, 30.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 160/160 [00:00<00:00, 1721.67 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1106.86 examples/s]\n",
      ">> Total approx train batches ( full | random ) : 201  (  97  |  104  )\n",
      "Map (num_proc=160): 100%|██████| 102912/102912 [00:47<00:00, 2171.63 examples/s]\n",
      ">> Saving dataset to data_path :  ../datapath/world/Eagle-x-multipack/\n",
      "Saving the dataset (41/41 shards): 100%|█| 103041/103041 [00:50<00:00, 2042.55 e\n",
      "Saving the dataset (1/1 shards): 100%|█| 64/64 [00:00<00:00, 2299.45 examples/s]\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 103,041\n",
      ">> Final dataset count ( test  ) : 64\n",
      ">> -----------------------------------\n",
      "Map (num_proc=160): 100%|██████| 103041/103041 [01:09<00:00, 1474.72 examples/s]\n",
      "num_proc must be <= 64. Reducing num_proc to 64 for dataset of size 64.\n",
      "Map (num_proc=64): 100%|████████████████| 64/64 [00:00<00:00, 190.67 examples/s]\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 3,354,691,235\n",
      ">> - Valid tokens : 2,073,274,062\n",
      ">> - Hidden tokens : 1,281,417,173\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 281,239\n",
      ">> - Valid tokens : 276,273\n",
      ">> - Hidden tokens : 4,966\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/{CONFIG_FILE_DIR}/{CONFIG_FILE_NAME}-build.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-02 18:49:43,863] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model][WARNING] - torch.compile is enabled, but this has been observed to perform worse, or even crash in some setup. Ensure to test if you actually measure speedups over JIT before using for large training runs'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-compile' with torch '2.1.2'\n",
      "/root/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/finetune-example/./Eagle-x-zMultipack.yaml', '--model.load_model=../model/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=../checkpoint/Eagle-x-zMultipack/', '--trainer.logger.init_args.name=RWKV-v5-Finetune - Eagle-x-zMultipack (tctxlen=4096, deepspeed_stage_2)', '--trainer.logger.init_args.project=RWKV-v5-Finetune', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=512', '--trainer.microbatch_size=8', '--model.ctx_len=4096', '--trainer.devices=auto'], args=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/finetune-example/./Eagle-x-zMultipack.yaml', '--model.load_model=../model/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=../checkpoint/Eagle-x-zMultipack/', '--trainer.logger.init_args.name=RWKV-v5-Finetune - Eagle-x-zMultipack (tctxlen=4096, deepspeed_stage_2)', '--trainer.logger.init_args.project=RWKV-v5-Finetune', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=512', '--trainer.microbatch_size=8', '--model.ctx_len=4096', '--trainer.devices=auto'].\n",
      "/root/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 190722569\n",
      "Seed set to 190722569\n",
      "/root/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Setup the checkpoint dir\n",
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"./checkpoint/{CONFIG_FILE_NAME}/\"\n",
    "\n",
    "# Lets start the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_NO_CUDA=1 && \\\n",
    "    export RWKV_TORCH_COMPILE=1 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/{CONFIG_FILE_DIR}/{CONFIG_FILE_NAME}.yaml\" \\\n",
    "        --model.load_model=\"../model/{MODEL_NAME}\" \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{CONFIG_FILE_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - {CONFIG_FILE_NAME} (tctxlen={TRAINING_CTX_LEN}, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.logger.init_args.project=\"{WANDB_PROJECT}\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.target_batch_size=512 \\\n",
    "        --trainer.microbatch_size={MICROBATCH_SIZE} \\\n",
    "        --model.ctx_len={TRAINING_CTX_LEN} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{CONFIG_FILE_NAME}/last.ckpt\" \"../model/{CONFIG_FILE_NAME}.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{CONFIG_FILE_NAME}.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check (that the model actually output stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 dragon_test.py \"../model/{CONFIG_FILE_NAME}.pth\" \"cuda bf16\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
