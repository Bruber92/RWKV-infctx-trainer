{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eagle 7B : Finetuning on textbooks!\n",
    "\n",
    "The following showcases an example of Training the RWKV-v5 7B model, on tiny-strange-textbooks\n",
    "- https://huggingface.co/datasets/nampdn-ai/tiny-strange-textbooks\n",
    "\n",
    "In this example, we will be training the model with 16k sample sizes\n",
    "\n",
    "## Configure the env variable below\n",
    "The default should work for 8x4090s, but tweak it accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_COUNT: 8\n",
      "GPU_0_VRAM_SIZE (GB): 23.64971923828125\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "DEEPSPEED_STRAT: deepspeed_stage_2\n",
      "TRAINING_CTX_LEN: 1024\n",
      "NOTEBOOK_DIR: /home/recursal/RWKV-infctx-trainer/notebook/finetune-example\n",
      "TRAINER_DIR: /home/recursal/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /home/recursal/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Your configurable settings\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# WANDB settings\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Eagle-X-Training\"\n",
    "WANDB_PROJECT=\"RWKV-v5-Finetune\"\n",
    "\n",
    "# Project directory offset (you need to modify if, you move the notebook into another dir)\n",
    "PROJECT_DIR_OFFSET=\"../../\"\n",
    "\n",
    "# Config dir (relative to the notebook, excluding ending slash)\n",
    "# to use, with the config filename\n",
    "CONFIG_FILE_DIR=\".\"\n",
    "CONFIG_FILE_NAME=\"Eagle-x-tiny-textbook\"\n",
    "\n",
    "# The model to use\n",
    "MODEL_NAME=\"RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\"\n",
    "MODEL_URL=\"https://huggingface.co/RWKV/v5-Eagle-7B/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth?download=true\"\n",
    "\n",
    "# GPU count to use\n",
    "GPU_DEVICES=\"auto\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Lets detect the GPU vram sizes, and suggest a resonable default\n",
    "# based on the detected VRAM sizes\n",
    "# -----------------------------------------------------------------\n",
    "import torch\n",
    "GPU_0_VRAM_SIZE_GB=torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "GPU_COUNT=torch.cuda.device_count()\n",
    "if GPU_DEVICES != \"auto\":\n",
    "    GPU_COUNT=int(GPU_DEVICES)\n",
    "print(\"GPU_COUNT:\", GPU_COUNT)\n",
    "print(\"GPU_0_VRAM_SIZE (GB):\", GPU_0_VRAM_SIZE_GB)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Auto select the strategy based on the detected VRAM size\n",
    "# -----------------------------------------------------------------\n",
    "if GPU_0_VRAM_SIZE_GB < 17:\n",
    "    assert False, \"For the Eagle-7B model, you need atleast 18GB vram\"\n",
    "elif GPU_0_VRAM_SIZE_GB < 23:\n",
    "    # This takes about 17.5GB vram on a single GPU\n",
    "    # We DO NOT recommend training with ctx_len=128, as the training\n",
    "    # quality will degrade noticably. But it will work!\n",
    "    DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "    TRAINING_CTX_LEN=128\n",
    "    MICROBATCH_SIZE=1\n",
    "elif GPU_0_VRAM_SIZE_GB < 25:\n",
    "    # This takes about 21GB vram on a single GPU\n",
    "    DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "    TRAINING_CTX_LEN=2048\n",
    "    MICROBATCH_SIZE=2\n",
    "elif GPU_0_VRAM_SIZE_GB < 50:\n",
    "    # This takes about 23GB vram on a single GPU\n",
    "    DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "    TRAINING_CTX_LEN=2048\n",
    "    MICROBATCH_SIZE=2\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# # Training settings you can use to override the \"auto\" default above\n",
    "# -----------------------------------------------------------------\n",
    "# DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "# TRAINING_CTX_LEN=4096\n",
    "# MICROBATCH_SIZE=8\n",
    "\n",
    "# ---\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"TRAINING_CTX_LEN:\", TRAINING_CTX_LEN)\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, PROJECT_DIR_OFFSET))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(TRAINER_DIR):\n",
    "    raise Exception(\"The trainer directory does not exists. Did you move the notebook?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-31 09:46:31--  https://huggingface.co/RWKV/v5-Eagle-7B/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 108.138.246.67, 108.138.246.71, 108.138.246.79, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.246.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/d5/6f/d56f8718b68e0e1840ad1e209498db64132d773e8c85a1bf4f194501bc3cddcf/a88c7274184b211e5545c8f992f0b80d03c40a447980bbfcd0f6d5858982615a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%3B+filename%3D%22RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%22%3B&Expires=1706953591&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjk1MzU5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q1LzZmL2Q1NmY4NzE4YjY4ZTBlMTg0MGFkMWUyMDk0OThkYjY0MTMyZDc3M2U4Yzg1YTFiZjRmMTk0NTAxYmMzY2RkY2YvYTg4YzcyNzQxODRiMjExZTU1NDVjOGY5OTJmMGI4MGQwM2M0MGE0NDc5ODBiYmZjZDBmNmQ1ODU4OTgyNjE1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Kjs6hOY4kufTg4jsPZufnr0QiJWgM5ZRjaDQgYwAfucK%7EDkbvq3Na7F-N2B1gn49Zfyr5DH6LDwRJUO-rExOonykbH818V52dzkQWF7PpAUwveJVGTLVuVnsfft54r6ePBmUDuN7pH6YZGTpmj%7Er4U07CoblI5MClmZDhYPDFd0xBSk41vINgNXUzakPXrZAd84G%7ECNo-ouYABS2amkQaqQxiFCVfyrNnwJw1%7EcwnrlK-wRn92W8ZF%7EjHs4cqrZsULB%7Ey%7EIEIcy5wN3Oq9m4zgxIboCO05nmKri%7EMACCdEvR2tEgvY1vUnhD8DDcMly8i2lOuSqnPEqyXHrzPbecMA__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-01-31 09:46:31--  https://cdn-lfs-us-1.huggingface.co/repos/d5/6f/d56f8718b68e0e1840ad1e209498db64132d773e8c85a1bf4f194501bc3cddcf/a88c7274184b211e5545c8f992f0b80d03c40a447980bbfcd0f6d5858982615a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%3B+filename%3D%22RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%22%3B&Expires=1706953591&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjk1MzU5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q1LzZmL2Q1NmY4NzE4YjY4ZTBlMTg0MGFkMWUyMDk0OThkYjY0MTMyZDc3M2U4Yzg1YTFiZjRmMTk0NTAxYmMzY2RkY2YvYTg4YzcyNzQxODRiMjExZTU1NDVjOGY5OTJmMGI4MGQwM2M0MGE0NDc5ODBiYmZjZDBmNmQ1ODU4OTgyNjE1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Kjs6hOY4kufTg4jsPZufnr0QiJWgM5ZRjaDQgYwAfucK%7EDkbvq3Na7F-N2B1gn49Zfyr5DH6LDwRJUO-rExOonykbH818V52dzkQWF7PpAUwveJVGTLVuVnsfft54r6ePBmUDuN7pH6YZGTpmj%7Er4U07CoblI5MClmZDhYPDFd0xBSk41vINgNXUzakPXrZAd84G%7ECNo-ouYABS2amkQaqQxiFCVfyrNnwJw1%7EcwnrlK-wRn92W8ZF%7EjHs4cqrZsULB%7Ey%7EIEIcy5wN3Oq9m4zgxIboCO05nmKri%7EMACCdEvR2tEgvY1vUnhD8DDcMly8i2lOuSqnPEqyXHrzPbecMA__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.173.121.43, 18.173.121.69, 18.173.121.15, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.173.121.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"./model\" && \\\n",
    "    cd \"./model\" && \\\n",
    "    wget -nc \"{MODEL_URL}\" -O \"{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=160): 100%|███████| 10000/10000 [00:00<00:00, 10826.81 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 10000/10000 [00:00<00:00, 14926.90 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 8061/8061 [00:01<00:00, 4165.24 examples/s]\n",
      "num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.\n",
      "Map (num_proc=83): 100%|████████████████| 83/83 [00:00<00:00, 215.06 examples/s]\n",
      "num_proc must be <= 83. Reducing num_proc to 83 for dataset of size 83.\n",
      "Map (num_proc=83): 100%|████████████████| 83/83 [00:00<00:00, 218.22 examples/s]\n",
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n",
      "Map: 100%|████████████████████████████████| 1/1 [00:00<00:00, 124.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 83/83 [00:00<00:00, 1259.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|████| 1/1 [00:00<00:00, 340.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && python3 preload_datapath.py \"{NOTEBOOK_DIR}/{CONFIG_FILE_DIR}/{CONFIG_FILE_NAME}.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-31 10:50:54,386] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/finetune-example/./Eagle-x-tiny-textbook.yaml', '--model.load_model=../model/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=../checkpoint/Eagle-x-tiny-textbook/', '--trainer.logger.init_args.name=Eagle-X-Training - Eagle-x-tiny-textbook (tctxlen=1024, deepspeed_stage_2)', '--trainer.logger.init_args.project=RWKV-v5-Finetune', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=64', '--trainer.microbatch_size=1', '--model.ctx_len=1024', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/finetune-example/./Eagle-x-tiny-textbook.yaml', '--model.load_model=../model/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth', '--data.skip_datapath_setup=True', '--trainer.callbacks.init_args.dirpath=../checkpoint/Eagle-x-tiny-textbook/', '--trainer.logger.init_args.name=Eagle-X-Training - Eagle-x-tiny-textbook (tctxlen=1024, deepspeed_stage_2)', '--trainer.logger.init_args.project=RWKV-v5-Finetune', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=64', '--trainer.microbatch_size=1', '--model.ctx_len=1024', '--trainer.devices=auto'].\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 721521523\n",
      "Seed set to 721521523\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       64\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         1\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    64\n",
      "\n",
      "[rank: 0] Seed set to 721521523\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-01-31 10:51:58,936] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-31 10:51:59,030] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-31 10:51:59,079] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-31 10:51:59,471] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-31 10:51:59,472] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-31 10:51:59,474] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-31 10:51:59,474] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[rank: 6] Seed set to 721521523\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 721521523\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 721521523\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 721521523\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 721521523\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 721521523\n",
      "[rank: 2] Seed set to 721521523\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Setup the checkpoint dir\n",
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"./checkpoint/{CONFIG_FILE_NAME}/\"\n",
    "\n",
    "# Lets start the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/{CONFIG_FILE_DIR}/{CONFIG_FILE_NAME}.yaml\" \\\n",
    "        --model.load_model=\"../model/{MODEL_NAME}\" \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{CONFIG_FILE_NAME}/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - {CONFIG_FILE_NAME} (tctxlen={TRAINING_CTX_LEN}, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.logger.init_args.project=\"{WANDB_PROJECT}\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.target_batch_size=64 \\\n",
    "        --trainer.microbatch_size={MICROBATCH_SIZE} \\\n",
    "        --model.ctx_len={TRAINING_CTX_LEN} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
