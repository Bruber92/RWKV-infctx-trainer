{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset repacking implementation\n",
    "\n",
    "Advance dataset operations, of sorting, offset, and length support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/recursal/RWKV-infctx-trainer/notebook/trainer-v5-validation\n",
      "TRAINER_DIR: /home/recursal/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /home/recursal/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"infctx-v5-datapack\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-28 02:00:22,765] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L6-D2048-world-v5base-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 6 --n_embd 2048 \\\n",
    "        --vocab_size world --skip-if-exists \\\n",
    "        \"../model/L6-D2048-world-v5base-init.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the datapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /home/recursal/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/datapack-build.yaml\n",
      ">> Preparing dataset - index:  0\n",
      "Map (num_proc=160): 100%|███████| 10000/10000 [00:00<00:00, 11515.81 examples/s]\n",
      "Filter (num_proc=160): 100%|████| 10000/10000 [00:00<00:00, 16988.70 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 9992/9992 [00:01<00:00, 5519.76 examples/s]\n",
      "Map (num_proc=160): 100%|███████████| 1258/1258 [00:01<00:00, 907.66 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 1258/1258 [00:00<00:00, 2062.82 examples/s]\n",
      "num_proc must be <= 13. Reducing num_proc to 13 for dataset of size 13.\n",
      "Map (num_proc=13): 100%|█████████████████| 13/13 [00:00<00:00, 59.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 1258/1258 [00:00<00:00, 18965.31 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█| 13/13 [00:00<00:00, 2478.00 examples/s]\n",
      ">> Preparing dataset - index:  1\n",
      "Map (num_proc=160): 100%|████| 242831/242831 [00:02<00:00, 121251.07 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 242831/242831 [00:01<00:00, 185706.79 examples/s]\n",
      "Map (num_proc=160): 100%|████| 240402/240402 [00:01<00:00, 121752.71 examples/s]\n",
      "Map (num_proc=160): 100%|████| 240402/240402 [00:01<00:00, 122685.82 examples/s]\n",
      "Map (num_proc=160): 100%|███████| 18199/18199 [00:00<00:00, 26847.06 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 2429/2429 [00:02<00:00, 1065.18 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 18199/18199 [00:00<00:00, 21397.84 exam\n",
      "Saving the dataset (1/1 shards): 100%|█| 2429/2429 [00:00<00:00, 73285.75 exampl\n",
      ">> Total approx train batches ( full | random ) : 9  (  7  |  2  )\n",
      "Map (num_proc=160): 100%|████████| 18432/18432 [00:03<00:00, 5626.16 examples/s]\n",
      ">> Final dataset sizes ( train ) : 19457\n",
      ">> Final dataset sizes ( test  ) : 2442\n",
      ">> Saving dataset to data_path :  ../datapath/v5-validation/example-datapack/\n",
      "Saving the dataset (1/1 shards): 100%|█| 19457/19457 [00:00<00:00, 23805.25 exam\n",
      "Saving the dataset (1/1 shards): 100%|█| 2442/2442 [00:00<00:00, 70412.54 exampl\n",
      ">> Dataset saved to data_path\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 datapack_build.py \"{NOTEBOOK_DIR}/config/datapack-build.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-28 06:58:53,174] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/datapack-train.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/datapack-validaiton-train/', '--trainer.logger.init_args.name=infctx-v5-datapack - Multi Datapack Validation - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.fast_dev_run=2', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/datapack-train.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/datapack-validaiton-train/', '--trainer.logger.init_args.name=infctx-v5-datapack - Multi Datapack Validation - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.fast_dev_run=2', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 2 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         8\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    64\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-01-28 06:59:01,214] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:01,241] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:01,242] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:01,250] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:01,288] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:01,332] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:01,384] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "[rank: 2] Seed set to 3941088705\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 5] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "[WARNING]: unlimited bptt_learning_range across multiple GPU's has a performance penalty with datasets of mixed sizes due to its constant need to keep all GPU's in sync (consider using bptt_learning_range=1 instead)\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.053250789642333984 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10134625434875488 seconds\n",
      "Time to load fused_adam op: 0.10124778747558594 seconds\n",
      "Time to load fused_adam op: 0.1014251708984375 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10177254676818848 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading extension module fused_adam...\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.10202813148498535 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.1022193431854248 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Time to load fused_adam op: 0.1019887924194336 seconds\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 134 M \n",
      "1 | blocks | ModuleList | 327 M \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 134 M \n",
      "--------------------------------------\n",
      "595 M     Trainable params\n",
      "0         Non-trainable params\n",
      "595 M     Total params\n",
      "2,383.086 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██████████████████| 2/2 [00:04<00:00,  0.43it/s, train/loss=9.620]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:00<00:00,  1.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 2/2 [00:01<00:00,  1.90it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 2/2 [00:06<00:00,  0.32it/s, train/loss=9.620, validation/loss=`Trainer.fit` stopped: `max_steps=2` reached.\n",
      "Epoch 0: 100%|█| 2/2 [00:06<00:00,  0.32it/s, train/loss=9.620, validation/loss=\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"disabled\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/datapack-train.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D2048-world-v5base-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/datapack-validaiton-train/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Multi Datapack Validation - (deepspeed_stage_1)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=8 \\\n",
    "        --trainer.fast_dev_run=2 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-28 06:59:29,981] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/test-dataset-repack.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/datapack-validaiton-train/', '--trainer.logger.init_args.name=infctx-v5-datapack - Multi Datapack Validation - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=auto'], args=['fit', '-c', '/home/recursal/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/test-dataset-repack.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/datapack-validaiton-train/', '--trainer.logger.init_args.name=infctx-v5-datapack - Multi Datapack Validation - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         8\n",
      "   - accumulate_grad_batches: 1\n",
      "   - effective_batch_size:    64\n",
      "\n",
      "[2024-01-28 06:59:38,169] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:38,194] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:38,207] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:38,215] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:38,218] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:38,232] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-01-28 06:59:38,422] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.2'\n",
      "[rank: 5] Seed set to 3941088705\n",
      "[rank: 4] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 1] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 2] Seed set to 3941088705\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/recursal/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/recursal/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "Map (num_proc=160):  97%|████████▊| 9724/10000 [00:04<00:00, 3232.47 examples/s][rank: 5] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "Map (num_proc=160): 100%|████████| 10000/10000 [00:05<00:00, 1771.12 examples/s]\n",
      "[rank: 7] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[rank: 1] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[rank: 3] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[rank: 2] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[rank: 4] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[rank: 6] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "Filter (num_proc=160): 100%|████| 10000/10000 [00:00<00:00, 14658.48 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 6409/6409 [00:01<00:00, 3414.39 examples/s]\n",
      "Map (num_proc=160): 100%|██████████| 6409/6409 [00:01<00:00, 4028.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 399/399 [00:00<00:00, 5874.66 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█| 65/65 [00:00<00:00, 3261.71 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "^C\n",
      "/home/recursal/miniconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/datapack-train.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D2048-world-v5base-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/datapack-validaiton-train/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Multi Datapack Validation - (deepspeed_stage_1)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=8 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
