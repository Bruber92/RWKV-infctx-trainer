{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset microbatch testing\n",
    "\n",
    "Testing runs on multiple micro batch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"infctx-v5-microbatch\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 11:19:39,010] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-world-v5base-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 6 --n_embd 512 \\\n",
    "        --vocab_size world --skip-if-exists \\\n",
    "        \"../model/L6-D512-world-v5base-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|█████████| 10000/10000 [00:01<00:00, 9575.14 examples/s]\n",
      "Filter (num_proc=16): 100%|█████| 10000/10000 [00:00<00:00, 12203.75 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 9892/9892 [00:00<00:00, 20646.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 9892/9892 [00:00<00:00, 241357.37 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 100/100 [00:00<00:00, 28064.93 examples\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_10k-world-full.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# microbatch=1\n",
    "\n",
    "Note: We are intentionally testing without rechunk, as that has known edge case issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 12:00:55,830] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-full.yaml', '--model.load_model=../model/L6-D512-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-full/', '--trainer.logger.init_args.name=infctx-v5-microbatch - Microbatch 1 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=1', '--trainer.devices=auto'], args=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-full.yaml', '--model.load_model=../model/L6-D512-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-full/', '--trainer.logger.init_args.name=infctx-v5-microbatch - Microbatch 1 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=1', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█| 9892/9892 [00:00<00:00, 595479.80 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 100/100 [00:00<00:00, 28472.64 examples\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory ../checkpoint/v5-enwiki-10k-full/ exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05255270004272461 seconds\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0:  16%|▏| 1600/9892 [00:55<04:49, 28.62it/s, v_num=mu7h, train/loss=5.310/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 9892/9892 [05:45<00:00, 28.62it/s, v_num=mu7h, train/loss=4.090\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 1/100 [00:00<00:30,  3.23it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 2/100 [00:00<00:28,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 3/100 [00:00<00:27,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 4/100 [00:01<00:27,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 5/100 [00:01<00:26,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 6/100 [00:01<00:26,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                | 7/100 [00:01<00:25,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                | 8/100 [00:02<00:25,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌                | 9/100 [00:02<00:25,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 10/100 [00:02<00:24,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 11/100 [00:03<00:24,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 12/100 [00:03<00:24,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 13/100 [00:03<00:23,  3.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 14/100 [00:03<00:23,  3.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 15/100 [00:04<00:23,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 16/100 [00:04<00:22,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 17/100 [00:04<00:22,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 18/100 [00:04<00:22,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 19/100 [00:05<00:22,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 20/100 [00:05<00:21,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 21/100 [00:05<00:21,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 22/100 [00:06<00:21,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 23/100 [00:06<00:21,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 24/100 [00:06<00:20,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 25/100 [00:07<00:22,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 26/100 [00:07<00:21,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 27/100 [00:07<00:21,  3.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 28/100 [00:08<00:21,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 29/100 [00:08<00:20,  3.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 30/100 [00:08<00:20,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 31/100 [00:08<00:20,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 32/100 [00:09<00:19,  3.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▌           | 33/100 [00:09<00:19,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 34/100 [00:09<00:19,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 35/100 [00:10<00:18,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 36/100 [00:10<00:18,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 37/100 [00:10<00:18,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 38/100 [00:10<00:17,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 39/100 [00:11<00:17,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 40/100 [00:11<00:17,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 41/100 [00:11<00:16,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 42/100 [00:11<00:16,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 43/100 [00:12<00:16,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 44/100 [00:12<00:15,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 45/100 [00:12<00:15,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 46/100 [00:13<00:15,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|███████▉         | 47/100 [00:13<00:15,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 48/100 [00:13<00:14,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 49/100 [00:13<00:14,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 50/100 [00:14<00:14,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 51/100 [00:14<00:13,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 52/100 [00:14<00:13,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████        | 53/100 [00:14<00:13,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 54/100 [00:15<00:12,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 55/100 [00:15<00:12,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 56/100 [00:15<00:12,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 57/100 [00:16<00:12,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 58/100 [00:16<00:11,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 59/100 [00:16<00:11,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 60/100 [00:16<00:11,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 61/100 [00:17<00:10,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 62/100 [00:17<00:10,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 63/100 [00:17<00:10,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▉      | 64/100 [00:17<00:10,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 65/100 [00:18<00:09,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▏     | 66/100 [00:18<00:09,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▍     | 67/100 [00:18<00:09,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|███████████▌     | 68/100 [00:18<00:08,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 69/100 [00:19<00:08,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 70/100 [00:19<00:08,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████     | 71/100 [00:19<00:08,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▏    | 72/100 [00:20<00:07,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|████████████▍    | 73/100 [00:20<00:07,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 74/100 [00:20<00:07,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████▊    | 75/100 [00:20<00:06,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▉    | 76/100 [00:21<00:06,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████    | 77/100 [00:21<00:06,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|█████████████▎   | 78/100 [00:21<00:06,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|█████████████▍   | 79/100 [00:21<00:05,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████▌   | 80/100 [00:22<00:05,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|█████████████▊   | 81/100 [00:22<00:05,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▉   | 82/100 [00:22<00:04,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████   | 83/100 [00:22<00:04,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|██████████████▎  | 84/100 [00:23<00:04,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|██████████████▍  | 85/100 [00:23<00:04,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|██████████████▌  | 86/100 [00:23<00:03,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|██████████████▊  | 87/100 [00:24<00:03,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████▉  | 88/100 [00:24<00:03,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|███████████████▏ | 89/100 [00:24<00:03,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|███████████████▎ | 90/100 [00:24<00:02,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|███████████████▍ | 91/100 [00:25<00:02,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|███████████████▋ | 92/100 [00:25<00:02,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|███████████████▊ | 93/100 [00:25<00:01,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████▉ | 94/100 [00:25<00:01,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|████████████████▏| 95/100 [00:26<00:01,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|████████████████▎| 96/100 [00:26<00:01,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|████████████████▍| 97/100 [00:26<00:00,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|████████████████▋| 98/100 [00:27<00:00,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|████████████████▊| 99/100 [00:27<00:00,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 100/100 [00:27<00:00,  3.63it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 9892/9892 [06:13<00:00, 26.50it/s, v_num=mu7h, train/loss=4.090`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 9892/9892 [06:13<00:00, 26.50it/s, v_num=mu7h, train/loss=4.090\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-full.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D512-world-v5base-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-10k-full/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Microbatch 1 - (deepspeed_stage_1)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=1 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# microbatch=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 12:11:50,734] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-full.yaml', '--model.load_model=../model/L6-D512-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-full/', '--trainer.logger.init_args.name=infctx-v5-microbatch - Microbatch 2 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=2', '--trainer.devices=auto'], args=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-full.yaml', '--model.load_model=../model/L6-D512-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-full/', '--trainer.logger.init_args.name=infctx-v5-microbatch - Microbatch 2 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=2', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         2\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█| 9892/9892 [00:00<00:00, 530522.66 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 100/100 [00:00<00:00, 28455.25 examples\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory ../checkpoint/v5-enwiki-10k-full/ exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05180692672729492 seconds\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0:  16%|▏| 800/4946 [00:35<03:05, 22.41it/s, v_num=3o87, train/loss=5.060]/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 4946/4946 [03:42<00:00, 22.19it/s, v_num=3o87, train/loss=5.720\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 1/100 [00:00<00:30,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 2/100 [00:00<00:28,  3.41it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 3/100 [00:00<00:27,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 4/100 [00:01<00:26,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                 | 5/100 [00:01<00:26,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 6/100 [00:01<00:26,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                | 7/100 [00:01<00:25,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                | 8/100 [00:02<00:25,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌                | 9/100 [00:02<00:25,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 10/100 [00:02<00:24,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 11/100 [00:03<00:24,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 12/100 [00:03<00:24,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 13/100 [00:03<00:23,  3.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 14/100 [00:03<00:23,  3.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 15/100 [00:04<00:23,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 16/100 [00:04<00:22,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 17/100 [00:04<00:22,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 18/100 [00:04<00:22,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 19/100 [00:05<00:22,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 20/100 [00:05<00:21,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 21/100 [00:05<00:21,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 22/100 [00:05<00:21,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 23/100 [00:06<00:20,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 24/100 [00:06<00:20,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 25/100 [00:07<00:22,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 26/100 [00:07<00:21,  3.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 27/100 [00:07<00:21,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 28/100 [00:08<00:20,  3.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 29/100 [00:08<00:20,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 30/100 [00:08<00:20,  3.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 31/100 [00:08<00:19,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 32/100 [00:09<00:19,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▌           | 33/100 [00:09<00:19,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 34/100 [00:09<00:18,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 35/100 [00:10<00:18,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 36/100 [00:10<00:18,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 37/100 [00:10<00:17,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 38/100 [00:10<00:17,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▋          | 39/100 [00:11<00:17,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 40/100 [00:11<00:17,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 41/100 [00:11<00:16,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▏         | 42/100 [00:11<00:16,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 43/100 [00:12<00:16,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 44/100 [00:12<00:15,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▋         | 45/100 [00:12<00:15,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 46/100 [00:12<00:15,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|███████▉         | 47/100 [00:13<00:14,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▏        | 48/100 [00:13<00:14,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 49/100 [00:13<00:14,  3.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 50/100 [00:14<00:14,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 51/100 [00:14<00:13,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▊        | 52/100 [00:14<00:13,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████        | 53/100 [00:14<00:13,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 54/100 [00:15<00:12,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▎       | 55/100 [00:15<00:12,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 56/100 [00:15<00:12,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 57/100 [00:15<00:12,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▊       | 58/100 [00:16<00:11,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 59/100 [00:16<00:11,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 60/100 [00:16<00:11,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▎      | 61/100 [00:17<00:10,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 62/100 [00:17<00:10,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 63/100 [00:17<00:10,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▉      | 64/100 [00:17<00:10,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 65/100 [00:18<00:09,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▏     | 66/100 [00:18<00:09,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▍     | 67/100 [00:18<00:09,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|███████████▌     | 68/100 [00:18<00:08,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 69/100 [00:19<00:08,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 70/100 [00:19<00:08,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████     | 71/100 [00:19<00:08,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▏    | 72/100 [00:19<00:07,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|████████████▍    | 73/100 [00:20<00:07,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 74/100 [00:20<00:07,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████▊    | 75/100 [00:20<00:06,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▉    | 76/100 [00:21<00:06,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████    | 77/100 [00:21<00:06,  3.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|█████████████▎   | 78/100 [00:21<00:06,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|█████████████▍   | 79/100 [00:21<00:05,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████▌   | 80/100 [00:22<00:05,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|█████████████▊   | 81/100 [00:22<00:05,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▉   | 82/100 [00:22<00:04,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████   | 83/100 [00:22<00:04,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|██████████████▎  | 84/100 [00:23<00:04,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|██████████████▍  | 85/100 [00:23<00:04,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|██████████████▌  | 86/100 [00:23<00:03,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|██████████████▊  | 87/100 [00:23<00:03,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████▉  | 88/100 [00:24<00:03,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|███████████████▏ | 89/100 [00:24<00:03,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|███████████████▎ | 90/100 [00:24<00:02,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|███████████████▍ | 91/100 [00:25<00:02,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|███████████████▋ | 92/100 [00:25<00:02,  3.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|███████████████▊ | 93/100 [00:25<00:01,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████▉ | 94/100 [00:25<00:01,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|████████████████▏| 95/100 [00:26<00:01,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|████████████████▎| 96/100 [00:26<00:01,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|████████████████▍| 97/100 [00:26<00:00,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|████████████████▋| 98/100 [00:26<00:00,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|████████████████▊| 99/100 [00:27<00:00,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████| 100/100 [00:27<00:00,  3.64it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 4946/4946 [04:10<00:00, 19.74it/s, v_num=3o87, train/loss=5.720`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 4946/4946 [04:10<00:00, 19.74it/s, v_num=3o87, train/loss=5.720\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-full.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D512-world-v5base-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-10k-full/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Microbatch 2 - (deepspeed_stage_1)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=2 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# microbatch=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 12:17:38,330] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-full.yaml', '--model.load_model=../model/L6-D512-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-full/', '--trainer.logger.init_args.name=infctx-v5-microbatch - Microbatch 2 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=auto'], args=['fit', '-c', '/home/picocreator/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_10k-world-full.yaml', '--model.load_model=../model/L6-D512-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-full/', '--trainer.logger.init_args.name=infctx-v5-microbatch - Microbatch 2 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█| 9892/9892 [00:00<00:00, 516553.02 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 100/100 [00:00<00:00, 28147.80 examples\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory ../checkpoint/v5-enwiki-10k-full/ exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05247139930725098 seconds\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0:   1%|  | 18/1237 [00:05<06:23,  3.18it/s, v_num=rhl5, train/loss=8.250]Traceback (most recent call last):\n",
      "  File \"/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/lightning_trainer.py\", line 296, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/lightning_trainer.py\", line 271, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 386, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 677, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 544, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 102, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 989, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1035, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 202, in run\n",
      "    self.advance()\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 359, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 136, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 240, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 187, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 265, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1282, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 151, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 264, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 230, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/deepspeed.py\", line 123, in optimizer_step\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 126, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 315, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 381, in training_step\n",
      "    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 633, in __call__\n",
      "    wrapper_output = wrapper_module(*args, **kwargs)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1818, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 626, in wrapped_forward\n",
      "    out = method(*_args, **_kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/src/model.py\", line 1234, in training_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, True)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/src/model.py\", line 1132, in compute_loss\n",
      "    self.manual_backward(learning_loss, optimizer, retain_graph=True)\n",
      "  File \"/home/picocreator/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/src/model.py\", line 779, in manual_backward\n",
      "    self.trainer.strategy.backward(loss, None, *args, **kwargs)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 204, in backward\n",
      "    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/deepspeed.py\", line 112, in backward\n",
      "    deepspeed_engine.backward(tensor, *args, **kwargs)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1940, in backward\n",
      "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1953, in backward\n",
      "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "    scaled_loss.backward(retain_graph=retain_graph)\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 0 has a total capacty of 22.16 GiB of which 1.03 GiB is free. Including non-PyTorch memory, this process has 21.09 GiB memory in use. Of the allocated memory 16.32 GiB is allocated by PyTorch, and 4.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-full.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D512-world-v5base-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-10k-full/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Microbatch 2 - (deepspeed_stage_1)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=4 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
